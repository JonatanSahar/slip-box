
*** draft:
***** what is statistical learning
              Combinatorial semantics is the idea that the meaning of words is learn-able through, and perhaps in part consisting of, their statistical relationship to other words in the language - specifically the frequency of their co-occurrence.
              On this view semantics of concepts learned this way do not necessarily require experience, since they're based in the relation between preexisting concepts rather than sensory and motor experiences. This relates more to so called secondary semantics, which are based on (derive their meaning from) those preexisting concepts which presumably /are/ grounded in experience ([[cite:&pulvermullerHowNeuronsMake2013]]).

              Algorithmic models which are meant to support this idea typically scan large corpuses and construct vector representations of words based on the frequency of their occurrence and their proximity to certain other words. I this way it has been shown that clusters of words can be created, which share semantic characteristics that we (as humans) would recognize as meaningful. [[cite:&pulvermullerHowNeuronsMake2013]]
               - The biological basis of this kind of representation is the idea hebbian learning - specifically  coactivation of circuits in language areas cortex (sTC) and inferior frontal cortex (iFC) on the rim of the left-perisylvian language center ([[cite:&pulvermullerHowNeuronsMake2013]], [[cite:&pulvermullerDiscreteCombinatorialCircuits2009]], [[cite:&honkelaContextualRelationsWords]]).

***** what is unification/integration
            Semantic unification is "the integration of word meaning into an unfolding representation of the preceding context" ([[cite:&giosubaggioSemanticUnification]]), it's the process by which an encountered word or complete sentence attains its full meaning within its semantic context, becomes disambiguated, its specific meaning chosen from the multiple possibilities we know it to posses.
            It's the major process by which longer stretched of text become coherent - their structure rendered meaningful.
            In terms of brain function, for several decades now the ERP N400 component has been associated with the effort of integrating the new information present in the latest word encountered ([[cite:&kutasThirtyYearsCounting2011]]), being more pronounced for words which are further from what is typically expected from the context.
            It has been seen to incorporate (be sensitive to) world-knowledge (e.g. statements which are false incur a more negative N400, and so do objects which are incongruous to their environment . [[cite:&hagoortIntegrationWordMeaning2004]] [[cite:&mudrikInformationIntegrationAwareness2014;&mudrikIntegrationAwarenessExpanding2011]]), and to be based on the current, transitive, context (statements which are surprising in typical context may be normal in the context of a story, representing a sort of neural correlate of a suspension of disbelief, [[cite:&vanberkumRightWrongBrain2009]])

how the paper informs them





bibliography:/home/jonathan/.bibliography/motor-cognition.bib
