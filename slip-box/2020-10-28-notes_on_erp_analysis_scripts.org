:PROPERTIES:
:ID:       20210627T195254.310114
:END:
#+TITLE: Notes on ERP analysis scripts
 - source :: [[file:/mnt/c/Users/Jonathan/Google Drive/Msc neuroscience/lab/analysis_scripts/erp-decoding/erp_decode.m::save(strcat(outputDir, "/", allSuccessRates.mat), 'allSuccessRates');][erp_decode.m script]]

* Thoughts about the project:
:PROPERTIES:
:ID:       a49cc007-75cc-4e89-b820-a0dc8e327051
:END:

*** overall average success rate: 35.2%
***** for 3-way ECOC
***** all results:

   | subject | avg success rate |
   |---------+------------------|
   |      11 |          49.9566 |
   |      13 |          34.8307 |
   |      14 |          34.0929 |
   |      *15* |          *25.6293* |
   |      *16* |          *26.8880* |
   |      17 |          36.0460 |
   |      19 |          31.3368 |
   |      20 |          40.4297 |
   |      21 |          37.1528 |

*** possible areas to improve:
***** SVM doesn't look at the signal over time
***** SVM trains on a lot of points with no relevant data, and has no way to learn to ignore them
***** SVM is tested on a lot of points it can't classify, so success rate is lower than it can be
***** how many ERPs should I average per subject? ~15? a balance between signal/noise ratio and amount of data
***** losing examples due to equating between conditions
*** possible ways to improve:
***** expand sliding window into $64 \cdot windowSize$ features
***** put effort into coding variable number of trials between conditions
***** try optimizing the ECOC learner
***** deep NN:
******* convolutional
******* recurring?
* pseudo code/walkthrough - my script
*** variables in the brainView output:
***** EEG - the EEG data + metadata
******* trials - #trials
******* mbchan - #electrodes
******* pnts - #time_points
******* times - the sampled time points (512 Hz)
******* data - the epochs, dimentions: #elctrodes x #time_points x #trials
******* chanlocs - (struct) names of the different electrodes
*** pre-processing steps:
***** get data from all the files for a give subject
***** create the labels array per subject per type
***** define "enum" for labels
***** remove all the irrelevant fields:

    ( 'filename', 'filepath', 'subject', 'group', 'condition', 'session', 'comments', 'srate', 'xmin', 'xmax', 'icaact', 'icawinv', 'icasphere', 'icaweights', 'icachansind', 'chanlocs', 'urchanlocs', 'chaninfo', 'ref', 'event', 'urevent', 'eventdescription', 'epoch', 'epochdescription', 'reject', 'stats', 'specdata', 'specicaact', 'splinefile', 'icasplinefile', 'dipfit', 'history', 'saved', 'etc', 'run', 'datfile')

***** load all the relevant fields into a class like svmECOC
*** truncate data to the length of the condition with fewest trials
*** decide on the number of trials per block - the minimum of trials/#blocks for the two conditions being tested
*** lowpass
*** downsample x4
*** iterate n=10 times
***** shuffle each <data,lables> pair independently of the others
***** divide ERPs into blocks
***** average ERPs in each block - gives the input matrix for that block: #elctrodes x #time_points
***** get the sliding window for every electrode - use  https://uk.mathworks.com/help/matlab/ref/movmean.html
***** construct the labels vector for each block
***** iterate CV folds = 3
******* give 2 blocks to the SVM, test with the 3rd
* original script
*** data files need to be in the in the same dir as the script
*** in the paper the data were bandpassed and corrected for blinks/eye movements using ICA
*** are channels the sa,e as electrodes? why do they refer to them as the tip locations as well? why as bins as well?
*** what's the structure of data.eeg? is 27 the number of electrodes?
*** prepare matrices for predictors:

    we have a prediction value (=certainty) per possible outcome (=channels =orientation), in each CV blocks,for every time point (since we're training a predictor per time point), per iteration

*** nTrials = # of epochs extracted for that subject
*** posbin = the labels for all the trials
*** shuffbin = the labels, shuffled
*** shuffBlocks = same length as # of trials
*** pseudo code:
***** set all the variables according to the metadata
***** lowpass the eeg data
***** for each iteration
******* something with the blocks
******* count the number of trials having each "ground truth" value
******* decide the number of trials that go into every bin so it'll be possible to have the same number of examples of every kind in every bin - that's nPerBin
******* shuffle the trial labels
******* for every label:
********* get all the indexes of that label (they are shuffled)
********* get the first nPerBin
********* for every trial, assign a block number (the trial numbers index the shuffBlocks array)
******* get the shuffBlocks array into the same order as the original labels array
******* and save the blocks assignment into the SVM class
******* average the data of all ERPs with the same label, that belong to the same block, at the relevant times (tois)
******* construct the labels vector and the block assignment for the averaged ERPs
******* for every time point:
********* get the points inside the window where t is the middle point
********* average across the window
********* train SVM on every combination of two blocks out of three, testing on the third.
********* save the predictions and true labels
